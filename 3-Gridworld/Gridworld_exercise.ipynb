{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld\n",
    "\n",
    "*Andrea Mazzolini*, andrea.mazzolini.90@gmail.com.\n",
    "\n",
    "\n",
    "Here we want to find the optimal strategy of a 2d grid-world problem having no information about the environment. The\n",
    "**Q-learning** algorithm will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld as a Markov Decision Process\n",
    "\n",
    "### States\n",
    "The state space corresponds to the physical space of the gridworld. Therefore each state is identified by the two coordinates, and the whole space is composed of $d^2$ states:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\{ 0, 1, \\ldots, d-1 \\} \\times \\{ 0, 1, \\ldots, d-1 \\}\n",
    "$$\n",
    "\n",
    "### Actions\n",
    "The actions of the agent are five: he can move to nearest neighbours or stay in the cell without moving:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\{ \\text{up}, \\text{left}, \\text{down}, \\text{right}, \\text{stay} \\} = \\{ (0,1), (-1,0), (0,-1), (1,0), (0,0)\\}\n",
    "$$\n",
    "\n",
    "which can be expressed also translation vectors.\n",
    "Actually, these actions are not always possible in each state: the agent cannot cross boundaries. This makes actions state dependent, for example if the agent is located on the left boundary: $\\mathcal{A}(0,y) = \\{ \\text{up}, \\text{down}, \\text{right}, \\text{stay} \\}$, or in a corner: $\\mathcal{A}(d-1,d-1) = \\{ \\text{left}, \\text{down}, \\text{stay} \\}$.\n",
    "\n",
    "### Transition probabilities\n",
    "\n",
    "The transition probabilities between states are deterministic: the next state is just the old state plus the translation action chosen by the agent:\n",
    "\n",
    "$$\n",
    "p(s_{t+1} | a_t, s_t) = \\delta (s_{t+1} = a_t + s_t)\n",
    "$$\n",
    "\n",
    "### Rewards\n",
    "\n",
    "The rewards depends only on the arrival states, $r(s_{t+1})$, and are zero for all the states with the exception of some special ones chosen to contain some resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning is a reinforcement learning algorithm for any finite Markov decision processes (state and action space must be discrete and finite), which can converge to an optimal policy for maximizing an exponentially discounted return.\n",
    "It does not require a model (hence the connotation \"model-free\") of the environment.\n",
    "For a \"model\" we mean the knowledge of the transition probabilities and reward function of the MDP. Differently value iteration or dynamic programming rely on this information.\n",
    "\n",
    "### Table for the state-action qualitites\n",
    "\n",
    "The general idea of the algorithm is to build a table of estimates of \"goodness\" of each state and action pairs. This object is called Quality matrix: ${Q}(s, a)$. It can be proven that, when the algorithm converge, the Qualitiy becomes the best quality function:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\rightarrow Q^*(s,a) = \\max_{\\pi} \\left[ \\mathbb{E}_\\pi\\left[ \\sum_{t=0}^\\infty \\gamma^t\\,r_t \\Big| s_0 = s, a_0 = a \\right] \\right]\n",
    "$$\n",
    "\n",
    "which is the best possible return starting from the state $s$ and taking the action $a$.\n",
    "By assuming that $Q$ are good estimates of $Q^*$, the best policy is deterministic and, for each state $s$, consists in choosing the action that leads to the best possible return:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\delta (a - \\text{argmax}_b Q(s,b))\n",
    "$$\n",
    "\n",
    "### Finding the quality matrix\n",
    "\n",
    "The core of the algorithm is a simple online quality update. At time $t$ the learning agent is in the state $s_t$ and take the action $a_t$ (later we specify how to choose the action). As a consequence it moves to a new state $s_{t+1}$ taking the reward $r_t$. Note that $s_{t+1}$ and $r_t$ are stochastic outcomes of the MDP, that the agent can just sample (it doesn't have a model and then a prediction of what they are).\n",
    "The Q-learning update rule for the Quality is:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(r_t + \\gamma \\max_b Q(s_{t+1}, b) - Q(s_t, a_t)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t)^* = \\mathbb{E} \\left[ r_t + \\gamma \\max_b Q^*(s_{t+1}, b) \\right] \n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the discount factor defined in the return that one aims to maximize, and $\\alpha$ is a learning rate.\n",
    "Note that the Bellman equation for the quality reads $Q(s_t, a_t)^* = \\mathbb{E} \\left[ r_t + \\gamma \\max_b Q(s_{t+1}, b) \\right] $, and the error appearing in the learning rule above says exactly how much I am far away from satisfying the equation with my sample. \n",
    "\n",
    "### Pseudocode for Q-learning\n",
    "\n",
    "Therefore, the algorithm consists simply in starting from an initial configuration fo the Quality table, and then \"play the game\"\n",
    "$$\n",
    "s_0, a_0 \\rightarrow r_0, s_1, a_1 \\rightarrow r_1, s_2, a_2 \\rightarrow \\ldots\n",
    "$$\n",
    "and update the Quality as specified above after each transition.\n",
    "\n",
    "We still have to specify how to choose actions. Here we consider an epsilon-greeedy strategy. Let us define two way of choosing the action:\n",
    "- **Exploration** move, where the action is choosen uniformely at random among the possible action from the state in which the aget is.\n",
    "- **Exploitation** move, where the action is taken as the one that maximize my current Qualitites, which is the best action that I can take according to my estimates of the returns, $a_t = \\text{argmax}_b Q(s_t, b)$.\n",
    "\n",
    "An epsilon-greeedy strategy says that the exploration move is chosen with probability $\\epsilon$, and the exploitation one otherwise.\n",
    "\n",
    "Putting everything together, the pseudocode for an epsilon-greedy Q-learning algorithm is the following:\n",
    "\n",
    " - Initialize the Q-matrix and choose the algorithm parameters $\\gamma$, $\\alpha$, $\\epsilon$.\n",
    " - Set the agent in the starting state $s_0$.\n",
    " - For $t = 1, \\ldots$ until convergence:\n",
    "> - With probability $\\epsilon$ choose $a_t$ at random from the possible actions, otherwise choose the action that maximizes the Qualities $a_t = \\text{argmax}_b Q(s_t, b)$.\n",
    "> - Play a step in the game and get the new state and the reward $s_t, a_t \\rightarrow s_{t+1}, r_t$\n",
    "> - Update the quality matrix using the obtained sample\n",
    "> $$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(r_t + \\gamma \\max_b Q(s_{t+1}, b) - Q(s_t, a_t)\\right)\n",
    "$$\n",
    "\n",
    "### Episodic game and exploration scheduling\n",
    "\n",
    "Actually one usually introduces two tricks to speed up the covergence of the quality table.\n",
    "\n",
    "The first is to restart the game after a given number of steps (the state is forced to be $s_0$ again). Each of these runs is called episode. This is natural if there are terminal states and, at some point, the game finishes. However, it can be useful to rerestart the game when I'm interested in a particular initial condition. In this way, I force the algorithm to explore more and have better estimates around this initial condition.\n",
    "\n",
    "A second trick is to schedule the exporation parameter $\\epsilon$. Usually, I want the exploration to be large at the beginning, to have an approximate idea of all the possible qualities. L\n",
    "ater I want instead to focus on the best moves to have more fine-tuned estimates of them, forgetting about the bad actions.\n",
    "\n",
    "Rewriting the pseudocode following these two observations we have:\n",
    "\n",
    " - Initialize the Q-matrix and choose the algorithm parameters $\\gamma$, $\\alpha$, $\\epsilon_0$, $T_{episode}$.\n",
    "> For episodes $e = 1, \\ldots$ until convergence:\n",
    "> - Set the agent in the starting state $s_0$.\n",
    "> - For steps in the episode $t = 1, \\ldots, T_{episode}$:\n",
    ">> - With probability $\\epsilon_e$ choose $a_t$ at random from the possible actions, otherwise choose the action that maximizes the Qualities $a_t = \\text{argmax}_b Q(s_t, b)$.\n",
    ">> - Play a step in the game and get the new state and the reward $s_t, a_t \\rightarrow s_{t+1}, r_t$\n",
    ">> - Update the quality matrix using the obtained sample\n",
    ">> $$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(r_t + \\gamma \\max_b Q(s_{t+1}, b) - Q(s_t, a_t)\\right)\n",
    "$$\n",
    "> - Decrease the exploration rate $\\epsilon_e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "To solve the problem, we suggest to use the following class for defining a gridworld environment. The exercise consists in finding the Q-matrix of a generic gridworld using the Q-learning algorithm above.\n",
    "\n",
    "At the end of the learning you should be able to find the best path from a starting state to the reward.\n",
    "\n",
    "\n",
    "### Environment class: the gridworld\n",
    "\n",
    "The Gridworld class contains all the information about the environment:\n",
    "- The info about the state space, the current state of the game and the initial state.\n",
    "- The set of possible actions.\n",
    "- The reward table: which reward the agent take in each cell (0 if none).\n",
    "\n",
    "The methods are:\n",
    "- `reset()`: the game is initialized. Here the only initialization is to put the agent in the starting cell. \n",
    "- `step(action)`: update the agent state according to the `action` passed and compute reward. Returns the state after the transition (the movement) and the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import operator\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "class Gridworld:\n",
    "\n",
    "    def __init__(self, grid_size, init_cell, rewards, obstacles=[]):\n",
    "        \"\"\"\n",
    "        Training environment for reinforcement learning: gridworld.\n",
    "        Args:\n",
    "        - grid_size, (int, int): defining the size of the 2d lattice\n",
    "        - init_cell, (int, int): coordinates from 0 to size-1 from which the agent starts to play\n",
    "        - rewards, list((int, int), float): list of the coordinates and values of the rewards\n",
    "        - obstacles, lits((int, int)): list of the coordinates of the obstacles\n",
    "        \"\"\"\n",
    "\n",
    "        # Define state space\n",
    "        self.state = None  # current state of the game\n",
    "        self.state_dim = grid_size\n",
    "        self.init_state = init_cell\n",
    "        self.obstacles = obstacles\n",
    "        # Cells that are not obstacles\n",
    "        self.states = [(i,j) for i in range(self.state_dim[0]) for j in range(self.state_dim[1]) if (i,j) not in self.obstacles] \n",
    "\n",
    "        # Define action space\n",
    "        self.action_dim = (5,)  # up, right, down, left, stay\n",
    "        self.action_dict = {\"up\": 0, \"right\": 1, \"down\": 2, \"left\": 3, \"stay\": 4}\n",
    "        self.action_coords = [(0, 1), (1, 0), (0, -1), (-1, 0), (0, 0)]  # translations\n",
    "        self.actions_allowed = self._build_allowed_actions(obstacles)\n",
    "\n",
    "        # Define rewards table\n",
    "        self.R = self._build_rewards(rewards)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent state to its initial cell\"\"\" \n",
    "        self.state = self.init_state\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Update agent state\"\"\"\n",
    "        state_next = (self.state[0] + self.action_coords[action][0],\n",
    "                      self.state[1] + self.action_coords[action][1])\n",
    "        # Collect reward\n",
    "        reward = self.R[state_next]\n",
    "        # Update state\n",
    "        self.state = state_next\n",
    "        return state_next, reward\n",
    "\n",
    "\n",
    "    def _build_allowed_actions(self, obstacles):\n",
    "        actions_allowed = dict()\n",
    "        Nx, Ny = self.state_dim\n",
    "        for x in range(Nx):\n",
    "            for y in range(Ny):\n",
    "                # Actions not allowed at the boundaries\n",
    "                actions_allowed[(x,y)] = [self.action_dict[\"stay\"]] # The stay action is always allowed\n",
    "                if (y > 0):  \n",
    "                    actions_allowed[(x,y)].append(self.action_dict[\"down\"])\n",
    "                if (y < Ny - 1):  \n",
    "                    actions_allowed[(x,y)].append(self.action_dict[\"up\"])\n",
    "                if (x > 0):  \n",
    "                    actions_allowed[(x,y)].append(self.action_dict[\"left\"])\n",
    "                if (x < Nx - 1):  \n",
    "                    actions_allowed[(x,y)].append(self.action_dict[\"right\"])\n",
    "                actions_allowed[(x,y)] = np.array(actions_allowed[(x,y)], dtype=int)\n",
    "\n",
    "                # Actions not allowed because of obstacles\n",
    "                for o in obstacles:\n",
    "                    if (x+1,y) == o:\n",
    "                        actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"right\"]]\n",
    "                    if (x-1,y) == o:\n",
    "                        actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"left\"]]\n",
    "                    if (x,y+1) == o:\n",
    "                        actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"up\"]]\n",
    "                    if (x,y-1) == o:\n",
    "                        actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"down\"]]\n",
    "        return actions_allowed\n",
    "\n",
    "\n",
    "    def _build_rewards(self, rewards):\n",
    "        R = np.zeros(self.state_dim, dtype=float)\n",
    "        for rew in rewards:\n",
    "            R[rew[0]] = rew[1]\n",
    "        return R\n",
    "\n",
    "    def display(self, values=np.array([]), cmap=sns.dark_palette(\"red\", as_cmap=True), figsize=(7,6)):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        obstacle_mask = np.zeros(self.state_dim, dtype=bool)\n",
    "        for obs in obstacles:\n",
    "            obstacle_mask[obs[0], obs[1]] = True\n",
    "\n",
    "        if len(values)==0:\n",
    "            ax = sns.heatmap(obstacle_mask.T, cmap=cm.get_cmap(\"Greys\"), cbar=False, \n",
    "                       linewidths=0.1, linecolor='#222222')\n",
    "        else:\n",
    "            ax = sns.heatmap(values.T, mask=obstacle_mask.T, cmap=cmap, \n",
    "                       linewidths=0.1, linecolor='#222222', vmin=np.min(values[values != 0]))\n",
    "            ax.collections[0].colorbar.set_label(\"Value\", fontsize=14)\n",
    "\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('x', fontsize=14)\n",
    "        ax.set_ylabel('y', fontsize=14)\n",
    "        ax.scatter([start_cell[0]+0.5],[start_cell[1]+0.5], s=100, c='grey', label='Start')\n",
    "        \n",
    "        for rew in rewards:\n",
    "            ax.scatter([rew[0][0]+0.5],[rew[0][1]+0.5], s=200*rew[1], c='red', label='Reward:{}'.format(rew[1]), marker='*')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to build a gridworld "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 360x360 with 1 Axes>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x2101afdcda0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "world_size = (10,12) # dimension of the gridworld\n",
    "\n",
    "start_cell = (2,3)\n",
    "obstacles = [(2,5), (3,5), (4,5), (5,5), (5,4), (5,3), (5,2), (5,1), (5,0),\n",
    "             (7,4), (7,5), (7,6), (7,7)]\n",
    "rewards = [((8,6), 2), ((3,9), 0.5)]\n",
    "\n",
    "gridworld = Gridworld(world_size, start_cell, rewards, obstacles) # Building the world\n",
    "gridworld.display(figsize=(5,5)) # And showing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
