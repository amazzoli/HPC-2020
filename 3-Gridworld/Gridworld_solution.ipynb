{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld\n",
    "\n",
    "*Andrea Mazzolini, amazzoli@ictp.it, Como 2019. From a previous tutorial of  Jacopo Talamini.*\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Here we want to find the optimal strategy of a 2d grid-world problem using a model-free reinforcement-learning algorithm: **Q-learning**.\n",
    "\n",
    "A 2D gridworld problem is a 2d lattice having size $d \\times d$, in which an agent can navigate and look for rewards. At each step the agent can move in one of the four neighbours (or stay still). Some coordinates are special and contain rewards, which the agent aquires if it moves over them or stays in the cell.\n",
    "\n",
    "The agent starts from an initial position $s_0$ and has to move and aquire rewards in order to maximize an exponentially discounted return:\n",
    "$$\n",
    "V_\\pi(s_0) = \\mathbb{E}_\\pi\\bigg[ \\sum_{t=0}^\\infty \\gamma^t\\,r_t \\bigg] \\ .\n",
    "$$\n",
    "\n",
    "We consider a case in which there are two rewards $R_a > R_b$, placed in the gridworld, such that:\n",
    "\n",
    "*   $R_a$ is far from the starting position of the agent, $s_0$\n",
    "*   $R_b$ is close to $s_0$\n",
    "\n",
    "Therefore there will be a tradeoff depending on $\\gamma$ for which reward the agent chooses to reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld as a Markov Decision Process\n",
    "\n",
    "### States\n",
    "The state space corresponds to the physical space of the gridworld. Therefore each state is identified by the two coordinates, and the whole space is composed of $d^2$ states:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\{ 0, 1, \\ldots, d-1 \\} \\times \\{ 0, 1, \\ldots, d-1 \\}\n",
    "$$\n",
    "\n",
    "### Actions\n",
    "The actions of the agent are five: he can move to nearest neighbours or stay in the cell without moving:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\{ \\text{up}, \\text{left}, \\text{down}, \\text{right}, \\text{stay} \\} = \\{ (0,1), (-1,0), (0,-1), (1,0), (0,0)\\}\n",
    "$$\n",
    "\n",
    "which can be expressed also translation vectors.\n",
    "Actually, these actions are not always possible in each state: the agent cannot cross boundaries. This makes actions state dependent, for example if the agent is located on the left boundary: $\\mathcal{A}(0,y) = \\{ \\text{up}, \\text{down}, \\text{right}, \\text{stay} \\}$, or in a corner: $\\mathcal{A}(d-1,d-1) = \\{ \\text{left}, \\text{down}, \\text{stay} \\}$.\n",
    "\n",
    "### Transition probabilities\n",
    "\n",
    "The transition probabilities between states are deterministic: the next state is just the old state plus the translation action chosen by the agent:\n",
    "\n",
    "$$\n",
    "p(s_{t+1} | a_t, s_t) = \\delta (s_{t+1} = a_t + s_t)\n",
    "$$\n",
    "\n",
    "### Rewards\n",
    "\n",
    "The rewards depends only on the arrival states, $r(s_{t+1})$, and are zero for all the states with the exception of some special ones chosen to contain some resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
